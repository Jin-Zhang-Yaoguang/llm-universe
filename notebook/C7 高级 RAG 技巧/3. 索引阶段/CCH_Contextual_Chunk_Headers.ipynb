{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上下文块头 （CCH） 是一种创建包含更高级别上下文（例如文档级或章节级上下文）的块头的方法，并在嵌入这些块头之前将这些块头附加到块中。这为嵌入提供了文本内容和含义的更准确和完整的表示。在我们的测试中，此功能可显著提高检索质量。除了提高检索正确信息的速度外，CCH 还降低了不相关结果在搜索结果中的显示速度。这降低了 LLM 在下游聊天和生成应用程序中误解一段文本的速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开发人员在使用 RAG 时面临的许多问题都归结为：单个块通常不包含足够的上下文，无法被检索系统或 LLM 正确使用。这导致无法回答问题，更令人担忧的是，还会出现幻觉。具体场景有\n",
    "- Chunk 通常通过隐含的引用和代词来指代其主题。这会导致它们在应该检索的时候没有被检索，或者 LLM 无法正确理解它们。\n",
    "- 单个块通常只在整个部分或文档的上下文中才有意义，并且单独阅读时可能会产生误导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现步骤\n",
    "1. 上下文生成：使用 LLM 为文档生成描述性标题。具体实现为：利用LLM完成简单的prompt模版，对每一个chunk生成描述性标题。如果有足够描述性的文档标题，则可以直接使用这些标题。例如：简明的文档摘要、章节/子章节标题。\n",
    "2. 将生成chunk header 嵌入 chunk\n",
    "3. 在结果中返回chunk header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinzhang/Desktop/C7 高级 RAG 技巧/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import tiktoken\n",
    "import os \n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/ft2ksmt92hd5vbtf1kj60wqc0000gn/T/ipykernel_5077/3694944193.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-zh-v1.5')\n",
      "/Users/jinzhang/Desktop/C7 高级 RAG 技巧/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key\n",
    "\n",
    "pdf_path = \"data/pumpkin_book.pdf\"\n",
    "qa_path = 'data/train_dataset.json'\n",
    "embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "with open(qa_path, 'r', encoding='utf-8') as file:\n",
    "    qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
    "指令\n",
    "总结以下文档内容的标题是什么？\n",
    "\n",
    "您的回答直接输出内容标题且仅此而已。请不要回应其他内容。\n",
    "\n",
    "{document_title_guidance}\n",
    "\n",
    "{truncation_message}\n",
    "\n",
    "文档\n",
    "{document_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "TRUNCATION_MESSAGE = \"\"\"\n",
    "请注意，下面提供的文档文本仅为文档的前~{num_words}个词。这对于此任务来说已经足够。您的回答仍应与整个文档相关，而不仅仅是下面提供的文本。\n",
    "\"\"\".strip()\n",
    "\n",
    "MAX_CONTENT_TOKENS = 4000\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    实现文本清理函数\n",
    "\n",
    "    参数:\n",
    "        text: 需要清理的字段\n",
    "\n",
    "    返回:\n",
    "        清理完成后返回的字段\n",
    "    \"\"\"\n",
    "    # 删除每页开头与结尾标语及链接\n",
    "    text = re.sub(r'→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←', '', text)\n",
    "    text = re.sub(r'→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', '', text)\n",
    "    # 删除字符串开头的空格\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    # 删除回车\n",
    "    text = re.sub(r'\\n+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def make_llm_call(chat_messages: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    调用 OpenAI 语言模型的 API。\n",
    "\n",
    "    参数:\n",
    "        chat_messages (list[dict]): 用于聊天完成的消息字典列表。\n",
    "\n",
    "    返回:\n",
    "        str: 语言模型生成的响应。\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=chat_messages,\n",
    "        max_tokens=MAX_CONTENT_TOKENS,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    将内容截断为指定的最大token数。\n",
    "\n",
    "    参数:\n",
    "        content (str): 需要截断的输入文本。\n",
    "        max_tokens (int): 保留的最大token数。\n",
    "\n",
    "    返回:\n",
    "        tuple[str, int]: 包含截断后内容和令牌数量的元组。\n",
    "    \"\"\"\n",
    "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
    "\n",
    "def get_document_title(document_text: str, document_title_guidance: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    使用语言模型提取文档标题。\n",
    "\n",
    "    参数:\n",
    "        document_text (str): 文档的文本内容。\n",
    "        document_title_guidance (str, 可选): 提取标题的额外指导。默认为 \"\"。\n",
    "\n",
    "    返回:\n",
    "        str: 提取的文档标题。\n",
    "    \"\"\"\n",
    "    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)\n",
    "    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else \"\"\n",
    "\n",
    "    prompt = DOCUMENT_TITLE_PROMPT.format(\n",
    "        document_title_guidance=document_title_guidance,\n",
    "        document_text=document_text,\n",
    "        truncation_message=truncation_message\n",
    "    )\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    return make_llm_call(chat_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Title: 《机器学习》入门教材总结\n"
     ]
    }
   ],
   "source": [
    "example_text = '“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解， 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者'\n",
    "\n",
    "document_title = get_document_title(example_text)\n",
    "print(f\"Document Title: {document_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=2000, chunk_overlap=100, is_header=False):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 嵌入将 PDF 书籍编码为向量存储。\n",
    "\n",
    "    参数:\n",
    "        path: PDF 文件的路径。\n",
    "        chunk_size: 每个文本块的期望大小。\n",
    "        chunk_overlap: 连续块之间的重叠量。\n",
    "\n",
    "    返回:\n",
    "        包含内容的向量存储。\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径，加载PDF\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    \n",
    "    # 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "    pdf_pages = loader.load()\n",
    "    \n",
    "    # 第13页为南瓜书第一页正文，因此从13页开始,从倒数13页涉及敏感用语，因此从-13页结束\n",
    "    data_pages = pdf_pages[13:-13]\n",
    "\n",
    "    for page in data_pages:\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "\n",
    "    # 文档分块\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator='')\n",
    "\n",
    "    split_docs = text_splitter.split_documents(data_pages)\n",
    "\n",
    "    # 给每一个分块增加header\n",
    "    if is_header:\n",
    "        for split_doc in tqdm(split_docs):\n",
    "            document_title = get_document_title(split_doc.page_content)\n",
    "            split_doc.page_content = f\"文章标题: {document_title}\\n\\n{split_doc.page_content}\"\n",
    "\n",
    "    # 构建向量库\n",
    "    vectordb = Chroma.from_documents(documents=split_docs, embedding=embedding)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCHRetriever:\n",
    "    def __init__(self, chunk_size=2000, chunk_overlap=100):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "        self.embeddings = embedding\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    # 基于pdf构建向量数据库   \n",
    "    # 构建不包含header的db \n",
    "    def encode_pdf_to_vectorstore_wo_header(self, files_path):\n",
    "        self.vectorstore_wo_header = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap, is_header=False)\n",
    "    # 构建包含header的db\n",
    "    def encode_pdf_to_vectorstore_w_header(self, files_path):\n",
    "        self.vectorstore_w_header = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap, is_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [04:49<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = CCHRetriever()\n",
    "retriever.encode_pdf_to_vectorstore_wo_header(pdf_path)\n",
    "retriever.encode_pdf_to_vectorstore_w_header(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '请根据提供的上下文信息，解释什么是“泛化”能力，并给出一个具体的例子说明为何泛化能力是衡量机器学习模型好坏的关键。',\n",
       " 'answer': '泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。例如学习西瓜好坏时，假设训练集中共有3个样本：{(x1=(青绿;蜷缩),y1=好瓜),(x2=(乌黑;蜷缩),y2=好瓜),(x3=(浅白;蜷缩),y3=好瓜)}，同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”，如果应用算法A在此训练集上训练得到模型fa(x)，模型a学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”，再应用算法B在此训练集上训练得到模型fb(x)，模型fb(x)学到的规律是“只要根蒂蜷缩就是好瓜”，因此对于一个未见过的西瓜样本x=(金黄;蜷缩)来说，模型fa(x)给出的预测结果为“坏瓜”，模型fb(x)给出的预测结果为“好瓜”，此时我们称模型fb(x)的泛化能力优于模型fa(x)。通过以上举例可知，尽管模型fa(x)和模型fb(x)对训练集学得一样好，即两个模型对训练集中每个样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另一重要原因，这也就是机器学习领域常说的“数据决定模型的上限，而算法则是让模型无限逼近上限”。',\n",
       " 'page_num': 14}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = 1\n",
    "\n",
    "qa_pairs[test_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 模型评估与选择\\n\\n第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 学习算法的有效性与样本需求分析\\n\\n•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 151, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 学习算法的泛化误差与样本数量的关系\\n\\nE(h)>ϵ且bE(h)=0是互斥的，因此总的概率P(h∈H:E(h)>ϵ∧bE(h)=0)就是这些互斥事件之和，即P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11=|H|XiP\\x10E(hi)>ϵ∧bE(hi)=0\\x11<|H|(1−ϵ)m小于号依据公式(12.11)。第二个小于号实际上是要证明|H|(1−ϵ)m<|H|e−mϵ，即证明(1−ϵ)m<e−mϵ，其中ϵ∈(0,1]，m是正整数，推导如下：当ϵ=1时，显然成立，当ϵ∈(0,1)时，因为左式和右式的值域均大于0，所以可以左右两边同时取对数，又因为对数函数是单调递增函数，所以即证明mln(1−ϵ)<−mϵ，即证明ln(1−ϵ)<−ϵ，这个式子很容易证明：令f(ϵ)=ln(1−ϵ)+ϵ，其中ϵ∈(0,1)，f′(ϵ)=1−11−ϵ=0⇒ϵ=0取极大值0，因此ln(1−ϵ)<−ϵ也即|H|(1−ϵ)m<|H|e−mϵ成立。12.3.4式(12.13)的解释回到我们要回答的问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。根据式12.12，学习算法L生成的假设大于目标假设的ϵ近似的概率为P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11<|H|e−mϵ，因此学习算法L生成的假设落在目标假设的ϵ近似的概率为1−P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11≥1−|H|e−mϵ，这个概率我们希望至少是1−δ，因此1−δ⩽1−|H|e−mϵ⇒|H|e−mϵ⩽δ12.3.5式(12.14)的推导|H|e−mϵ⩽δe−mϵ⩽δ|H|−mϵ⩽lnδ−ln|H|m⩾1ϵ\\x12ln|H|+ln1δ\\x13这个式子告诉我们，在假设空间H是PAC可学习的情况下，输出假设h的泛化误差ϵ随样本数目m增大而收敛到0，收敛速率为O(1m)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集样本数量越多，机器学习模型的泛化性能越好。12.3.6引理12.1的解释根据式(12.2),bE(h)=1mPmi=1I(h(xi)̸=yi),而指示函数I(·)取值非0即1,也就是说0≤I(h(xi)̸=yi)≤1;对于式(12.1)的E(h)实际上表示I(h(xi)̸=yi)为1的期望E(I(h(xi)̸=yi))(泛化误差表示样本空间中任取一个样本,其预测类别不等于真实类别的概率),当假设h确定时,泛化误差固定不变,因此可记为E(h)=1mPmi=1E(I(h(xi)̸=yi))。此时,将bE(h)和E(h)代入式(12.15)到式(12.17),对比式(12.5)和式(12.6)的Hoeffding不等式可知,式(12.15)对应式(12.5),式(12.16)与式(12.15)对称,式(12.17)对应式(12.6)。')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = qa_pairs[test_doc]['query']\n",
    "results = retriever.vectorstore_wo_header.similarity_search(test_query, k=5)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 模型评估与选择\\n\\n第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 学习算法的有效性与样本需求分析\\n\\n•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 151, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 学习算法的泛化误差与样本数量的关系\\n\\nE(h)>ϵ且bE(h)=0是互斥的，因此总的概率P(h∈H:E(h)>ϵ∧bE(h)=0)就是这些互斥事件之和，即P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11=|H|XiP\\x10E(hi)>ϵ∧bE(hi)=0\\x11<|H|(1−ϵ)m小于号依据公式(12.11)。第二个小于号实际上是要证明|H|(1−ϵ)m<|H|e−mϵ，即证明(1−ϵ)m<e−mϵ，其中ϵ∈(0,1]，m是正整数，推导如下：当ϵ=1时，显然成立，当ϵ∈(0,1)时，因为左式和右式的值域均大于0，所以可以左右两边同时取对数，又因为对数函数是单调递增函数，所以即证明mln(1−ϵ)<−mϵ，即证明ln(1−ϵ)<−ϵ，这个式子很容易证明：令f(ϵ)=ln(1−ϵ)+ϵ，其中ϵ∈(0,1)，f′(ϵ)=1−11−ϵ=0⇒ϵ=0取极大值0，因此ln(1−ϵ)<−ϵ也即|H|(1−ϵ)m<|H|e−mϵ成立。12.3.4式(12.13)的解释回到我们要回答的问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。根据式12.12，学习算法L生成的假设大于目标假设的ϵ近似的概率为P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11<|H|e−mϵ，因此学习算法L生成的假设落在目标假设的ϵ近似的概率为1−P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11≥1−|H|e−mϵ，这个概率我们希望至少是1−δ，因此1−δ⩽1−|H|e−mϵ⇒|H|e−mϵ⩽δ12.3.5式(12.14)的推导|H|e−mϵ⩽δe−mϵ⩽δ|H|−mϵ⩽lnδ−ln|H|m⩾1ϵ\\x12ln|H|+ln1δ\\x13这个式子告诉我们，在假设空间H是PAC可学习的情况下，输出假设h的泛化误差ϵ随样本数目m增大而收敛到0，收敛速率为O(1m)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集样本数量越多，机器学习模型的泛化性能越好。12.3.6引理12.1的解释根据式(12.2),bE(h)=1mPmi=1I(h(xi)̸=yi),而指示函数I(·)取值非0即1,也就是说0≤I(h(xi)̸=yi)≤1;对于式(12.1)的E(h)实际上表示I(h(xi)̸=yi)为1的期望E(I(h(xi)̸=yi))(泛化误差表示样本空间中任取一个样本,其预测类别不等于真实类别的概率),当假设h确定时,泛化误差固定不变,因此可记为E(h)=1mPmi=1E(I(h(xi)̸=yi))。此时,将bE(h)和E(h)代入式(12.15)到式(12.17),对比式(12.5)和式(12.6)的Hoeffding不等式可知,式(12.15)对应式(12.5),式(12.16)与式(12.15)对称,式(12.17)对应式(12.6)。')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = qa_pairs[test_doc]['query']\n",
    "results = retriever.vectorstore_w_header.similarity_search(test_query, k=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果测评"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 39.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 不使用CCH\n",
    "from tqdm import tqdm\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for qa_pair in tqdm(qa_pairs):\n",
    "    if len(qa_pair['query']) > 10:\n",
    "        query = qa_pair['query']\n",
    "        sim_docs = retriever.vectorstore_wo_header.similarity_search(query, k=10)\n",
    "        page_nums = [doc.metadata['page'] for doc in sim_docs]\n",
    "        if qa_pair['page_num'] in page_nums: i += 1\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率为: 83.63636363636363%\n"
     ]
    }
   ],
   "source": [
    "print(f\"召回率为: {i/j * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 42.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 不使用CCH\n",
    "from tqdm import tqdm\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for qa_pair in tqdm(qa_pairs):\n",
    "    if len(qa_pair['query']) > 10:\n",
    "        query = qa_pair['query']\n",
    "        sim_docs = retriever.vectorstore_w_header.similarity_search(query, k=10)\n",
    "        page_nums = [doc.metadata['page'] for doc in sim_docs]\n",
    "        if qa_pair['page_num'] in page_nums: i += 1\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率为: 83.63636363636363%\n"
     ]
    }
   ],
   "source": [
    "print(f\"召回率为: {i/j * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
